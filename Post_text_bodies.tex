\appendix
\chapter{Multi-degree-of-freedom systems}

\section{Forced Vibration Solution}\label{sec:damped-mdof-frf}
\label{sec:frfappendix}
The following is the author's interpretation and summary of the work by Fu in \cite{fu2001modal}, Ewins in \cite{ewins2000modal}, and Rogers in \cite{Tim_FRF} and \cite{Tim_Modal_Properties2}
As previously highlighted in Section \ref{sec:intro}, the solution to a dynamic system can be represented by an eigenvalue problem:
\begin{equation}
    (\textbf{K}-\omega^{2}\textbf{M})\{\varphi\} = 0
    \label{eigen_new}
\end{equation}
Considering 2 modes, $i$ and $j$, equation \ref{eigen_new} becomes:

\begin{equation}
    (\textbf{K}-\omega^{2}_{i}\textbf{M})\{\varphi\}_{i} = 0
    \label{mode_i}
\end{equation}
\begin{center}
    and
\end{center}

\begin{equation}
    (\textbf{K}-\omega^{2}_{j}\textbf{M})\{\varphi\}_{j} = 0
    \label{mode_j}
\end{equation}

Pre-multiplying \ref{mode_i} by $\{\varphi\}_{j}^{T} $, and Transposing \ref{mode_j} and post-multiplying by $\{\varphi\}_{i}$ results in:
\begin{equation}
    \{\varphi\}_{j}^{T}(\textbf{K}-\omega^{2}_{i}\textbf{M})\{\varphi\}_{i} = 0
    \label{zeby1}
\end{equation}
\begin{center}
    and
\end{center}
\begin{equation}
    \{\varphi\}_{j}^{T}(\textbf{K}-\omega^{2}_{j}\textbf{M})\{\varphi\}_{i} = 0
    \label{zeby2}
\end{equation}
subtracting  \ref{zeby1} and \ref{zeby2}:

\begin{equation}
    (\omega^{2}_{i} -\omega^{2}_{j})\{\varphi\}_{j}^{T} \textbf{M} \{\varphi\}_{i} = 0
    \label{zeby3}
\end{equation}
suggesting that for $i \neq j$,  $\{\varphi\}_{j}^{T} \textbf{M} \{\varphi\}_{i} = 0$, substituting this property in equation \ref{zeby2}, $\{\varphi\}_{j}^{T} \textbf{K} \{\varphi\}_{i} = 0$ holds true.
Considering the case where $i = j$, let 
\begin{equation}
    \{\varphi\}_{i}^{T} \textbf{M} \{\varphi\}_{i} = m_{i} 
    \label{massmodal} 
\end{equation}
\begin{center}
    and
\end{center}
\begin{equation} 
    \{\varphi\}_{i}^{T} \textbf{K} \{\varphi\}_{i} = k_{i} 
    \label{modal_stiffness} 
\end{equation}
where $m_{i}$ and $k_{i}$ are the modal mass and stiffness respectively, the eigenvalue for each mode $\lambda_{i}$ can be represented as:
$$\lambda_{i} = \frac{k_{i}}{m_{i}}$$.
The properties highlighted by  equations \ref{zeby1}, \ref{zeby2}, \ref{zeby3}, \ref{massmodal} and \ref{modal_stiffness} are expressed in matrix form more concisely, where:
\begin{equation}
    [\Phi]^{T}\textbf{M} [\Phi] = \text{diag}(m_{i})
    \label{modal_matrix_mass}
\end{equation}
\begin{center}
    and
\end{center}
\begin{equation}
    [\Phi]^{T}\textbf{K} [\Phi] = \text{diag}(k_{i})
    \label{modal_matrix_stiff}
\end{equation}
Referring to the mathematical representation of the frequency response function, from equation \ref{FRF_general_form}, where $H(\omega) =  (\textbf{K}-\omega^{2}\textbf{M})^{-1}$, 
one can express the FRF of a system in terms of its modal parameters as follows,
\begin{equation}
    [\Phi]^{T} (\textbf{K}-\omega^{2}\textbf{M})[\Phi] =  [\Phi]^{T} [H(\omega)]^{-1} [\Phi]
\end{equation}
Substituting from \ref{modal_matrix_mass} and \ref{modal_matrix_stiff}, the FRF matrix is expressed as:
\begin{equation}
    [H(\omega)] = [\Phi][\omega_{i}^{2}-\omega^{2}][\Phi]^{T}
\end{equation}
For a single element in a receptance FRF matrix $H_{oj}(\omega)$, the equation representing it is:
\begin{equation}
    H_{jk}(\omega) = \frac{\varphi_{j1}\varphi_{k1}}{\omega_{1}^{2}-\omega^{2}} + \frac{\varphi_{j2}\varphi_{k2}}{\omega_{2}^{2}-\omega^{2}} + \dots + \frac{\varphi_{jn}\varphi_{kn}}{\omega_{n}^{2}-\omega^{2}}
    \label{FRF_MODAL}
\end{equation}
where $n$ is the number of modes. Equation \ref{FRF_MODAL} indicates that the value of the FRF at a single frequency line can be interpreted as 
the contribution of all individual modes to the specific vibration pattern associated with the frequency line. Essentially the FRF is represented as a sum of partial fractions.
This representation of the FRF is the basis of the validity of the pLSCF method, 
as it similarly assumes a rational fractional representation of the FRF.


\chapter{Derivation of p-LSCF modal parameter estimation method}\label{sec:POLYMAX-DERIVATION}
\section{The right-matrix rational fractional model}
The polyreference least-squares complex frequency domain method employs a right matrix fractional model to fit MIMO Frequency Response Function measurements into a set of rational polynomial transfer functions:
\begin{equation}\label{eq:plscf_poly}
    [H(\omega)] = [N(\omega)][D(\omega)]^{-1}
\end{equation}
Such that $H(\omega) \in\mathbb{C}^{N_{outputs} \times N_{inputs}} $ is the FRF matrix, where $D(\omega)\in \mathbb{C}^{N_{inputs} \times N_{inputs}}$, is the denominator matrix polynomial, and $N(\omega) \in \mathbb{C}^{N_{outputs} \times N_{inputs}}$, is the numerator matrix polynomial. The rows corresponding to each output $o$ in the FRF matrix can be represented as such:

\begin{equation}\label{eq:plscf_frf_row}
    \left \langle H_o(\omega) \right \rangle = \left \langle N_o(\omega) \right \rangle [D(\omega)]^{-1}
\end{equation}

The row vector numerator polynomial for the $o^{th}$ output, and the denominator matrix polynomial are defined in terms of a polynomial basis function, $\Omega(\omega)$, and their respective polynomial coefficients, $\beta$ and $\alpha$ as such:

\begin{equation}\label{eq:num_poly}
    \left \langle N_o(\omega) \right \rangle  = \sum_{r=1}^{p} \Omega_r(\omega) \left \langle \beta_{or}(\omega) \right \rangle
\end{equation}
\begin{equation}\label{eq:den_poly}
    [D(\omega)] = \sum_{r=1}^{p} \Omega_r(\omega)[\alpha_r]
\end{equation}
With the polynomial basis function $\Omega_r(\omega) = e^{j\omega\Delta tr}$. Although not initially obvious as polynomials with conventional form $p(x) = c_0 +c_1x +c_2x^2 +\dots + c_nx^n$, the basis functions are expressed in the $s$-domain where $s = e^{j\omega\Delta t}$. The polynomial coefficients, $\alpha_r \in \mathbb{R}^{N_{inputs}\times N_{inputs}}$ and $\beta_{or} \in \mathbb{R}^{1 \times N_{inputs}}$, are assembled into matrix form:

\begin{flalign}\label{eq:beta}
    \beta_o = \begin{pmatrix}
        \beta_{o0}\\
        \beta_{o1}\\
        \beta_{o2}\\
        \dots\\
        \beta_{op}
    \end{pmatrix}
    \in \mathbb{R}^{(p+1)\times N_{inputs}} &&
\end{flalign}


\begin{flalign}\label{eq:alpha}
    \alpha = \begin{pmatrix}
        \alpha_0\\
        \alpha_1\\
        \alpha_2\\
        \dots\\
        \alpha_p
    \end{pmatrix}
    \in \mathbb{R}^{N_{inputs}*(p+1)\times N_{inputs}} &&
\end{flalign}

\begin{flalign}\label{eq:theta}
    \theta = \begin{pmatrix}
        \beta_0\\
        \beta_1\\
        \beta_2\\
        \dots\\
        \beta_{N_{o}}\\
        \alpha
    \end{pmatrix} \in \mathbb{R}^{(N_{outputs}+N_{inputs})(p+1)\times N_{inputs}} &&
\end{flalign}
\section{Minimizing the sum of the squared residuals}
The collection of both sets of coefficients into one variable $\theta$, makes performing the least squares problem simpler, in a sense, as it becomes the one unknown in this least squares model. As typical in any fitting method, one must minimize the error between the model and the real or measured value. The nonlinear least-squares error for:\\
Measured FRF:\hspace{30pt} $\hat{H}_o(\omega_k)$\\
Model FRF: \hspace{42pt} $H_o(\omega_k)$\\
is weighted such that:
\begin{equation}\label{eq:nls_error}
    \epsilon_{o}^{NLS} (\theta,\omega_k) = w_o(\omega_k)(H_o(\omega_k)-\hat{H}_o(\omega_k))
\end{equation}
Where $\epsilon_{o}^{NLS} \in \mathbb{C}^{1\times N_{inputs}}$,   $w_o(\omega_k)$ is a scalar weighing function which captures the variation and deviation between multiple inputs on the same measurement point, and $\forall k = 0, 1, 2, \dots , N_{frequency}$. Said weighing function is typically denoted by
\begin{equation}\label{eq:weighing_fn}
    w_o(\omega_k) = \frac{1}{\sqrt{\mathbf{var}[H_o(\omega_k)]}}
\end{equation}
(See [reference for weighted linear regressions] for more information on weighted least squares.)\\
One can then define the nonlinear cost function as the sum of the error "squared", (hermitian inner product), over the data points, in this case, spectral lines and outputs;
\begin{equation} \label{eq:nls_cost_fn}
    l^{NLS}(\theta) = \sum_{o=1}^{N_{out}}\sum_{k=1}^{N_f}\mathbf{tr}\{(\epsilon_{o}^{NLS} (\theta,\omega_k))^H \epsilon_{o}^{NLS} (\theta,\omega_k)  \}
\end{equation}
In this equation, $\mathbf{tr}\{\bullet\}$ denotes the trace of a matrix, also known as the sum of diagonal elements, and $\bullet^H$ denotes the Hermitian (conjugate) transpose. The trace operator is used as the trace of a product of 2 matrices $\mathbf{A}\in \mathbb{C}^{m\times n}$ and $\mathbf{B}\in \mathbb{C}^{m\times n}$ will equal the sum of each individual element in $\mathbf{A}$ with the individual elements of $\mathbf{B}$. This provides a sum of all square residuals/errors in the cost function.
\begin{equation}\label{eq:tr_cyc_prop}
    \mathbf{tr}\{\mathbf{A}^H\mathbf{B}\} = \mathbf{tr}\{\mathbf{A}\mathbf{B}^H\} = \mathbf{tr}\{\mathbf{B}^H\mathbf{A}\} =\mathbf{tr}\{\mathbf{B}\mathbf{A}^H\} = \sum_{i=1}^{m}\sum_{j=1}^{n}a_{ij}b_{ij}
\end{equation}
\section{Linearizing the error}
One can then obtain the polynomial coefficients through minimizing the cost function in \ref{eq:nls_cost_fn}, by setting the derivative $\frac{\partial l^{NLS}}{\partial \theta}$ equal to zero, however a nonlinear cost function will yield nonlinear derivative equations, (typically called normal equations in linear regression). A subsequent linearization of the cost function can approximate (suboptimally) the least squares problem, this is achieved through right multiplying the cost function with denominator polynomial $\mathbf{D}$. This gives a linear error:
\begin{equation}\label{eq:lin_error}
    \begin{aligned}
        \epsilon^{LS}_o(\omega_k,\theta)  & = w_o(\omega_k) (N_o(\omega_k,\beta_o)-\hat{H}_o(\omega_k)D(\omega_k,\alpha))\\
        & = w_o(\omega_k)  \sum_{r=0}^{p}(\Omega_r(\omega_k)\beta_{or}-\Omega_r(\omega_k)\hat{H}_o(\omega_k)\alpha_r)
    \end{aligned}
\end{equation}
Stacking the error in terms for all spectral lines in one matrix $E^{LS}_o(\theta) \in \mathbb{C}^{N_f \times N_in}$:
\begin{equation}\label{eq:stacked_error}
    E^{LS}_o(\theta) = \begin{pmatrix}
        \epsilon^{LS}_o(\omega_1,\theta)\\
        \epsilon^{LS}_o(\omega_2,\theta)\\
        \epsilon^{LS}_o(\omega_3,\theta)\\
        \dots\\
        \epsilon^{LS}_o(\omega_{N_f},\theta)
    \end{pmatrix}
    = \begin{pmatrix}
        X_o & Y_o
    \end{pmatrix}
    \begin{pmatrix}
        \beta_o\\
        \alpha
    \end{pmatrix}
\end{equation}

Here, new variables $\mathbf{X}$ and $\mathbf{Y}$ are introduced:
\begin{equation}\label{eq:X}
    X_o = \begin{pmatrix}
        w_{o}(\omega_{1})\Bigl(\Omega_{0}(\omega_{1}) + \Omega_{1}(\omega_{1}) \dots \Omega_{p}(\omega_{1})\Bigr) \\
        w_{o}(\omega_{2})\Bigl(\Omega_{0}(\omega_{2}) + \Omega_{1}(\omega_{2}) \dots \Omega_{p}(\omega_{2})\Bigr) \\
        \vdots \\
        w_{o}(\omega_{N_{f}})\Bigl(\Omega_{0}(\omega_{N_{f}}) + \Omega_{1}(\omega_{N_{f}}) \dots \Omega_{p}(\omega_{N_{f}})\Bigr)
        \end{pmatrix}
        \in \mathbb{C}^{N_f \times (p+1)}
\end{equation}
\begin{equation}\label{eq:Y}
    Y_o = \begin{pmatrix}
        -w_{o}(\omega_{1})\Bigl(\Omega_{0}(\omega_{1}) + \Omega_{1}(\omega_{1}) \dots \Omega_{p}(\omega_{1})\Bigr) \otimes \hat{H}_o(\omega_1) \\ 
        -w_{o}(\omega_{2})\Bigl(\Omega_{0}(\omega_{2}) + \Omega_{1}(\omega_{2}) \dots \Omega_{p}(\omega_{2})\Bigr)\otimes \hat{H}_o(\omega_2)  \\
        \vdots \\
        -w_{o}(\omega_{N_{f}})\Bigl(\Omega_{0}(\omega_{N_{f}}) + \Omega_{1}(\omega_{N_{f}}) \dots \Omega_{p}(\omega_{N_{f}})\Bigr) \otimes \hat{H}_o(\omega_{N_f}) 
        \end{pmatrix}
        \in \mathbb{C}^{N_f \times N_{in}(p+1)}
\end{equation}
Where $\otimes$ is the Kronecker product. In these equations, $\mathbf{X}$ is used to capture the frequency content of the least squares problem, and $\mathbf{Y}$ is used to capture both the frequency content and the measured response data. Using these matrices, one can reconstruct the nonlinear cost function into one that is linear:
\begin{equation}
    \begin{aligned}\label{eq:lin_cost_fn}
        l^{LS}(\theta) & = \sum_{o=1}^{N_{out}}\sum_{k=1}^{N_{f}}\mathbf{tr}\{(\epsilon^{LS}_o(\omega_k,\theta))^H \epsilon^{LS}_o(\omega_k,\theta) \}\\
        & = \sum_{o=1}^{N_{out}}\mathbf{tr}\biggl\{(E^{LS}_o(\theta))^H E^{LS}_o(\theta) \biggr\}\\
        & = \sum_{o=1}^{N_{out}}\mathbf{tr} \Biggl\{\begin{pmatrix}
            \beta^{T}_o & \alpha^{T}
        \end{pmatrix}
        \begin{pmatrix}
            X^{H}_o\\Y^{H}_o
        \end{pmatrix}
        \begin{pmatrix}
            X_o&Y_o
        \end{pmatrix}
        \begin{pmatrix}
            \beta_o \\ \alpha
        \end{pmatrix}\Biggr\}
    \end{aligned}
\end{equation}
If one defines a \textit{Jacobian} matrix $\mathbf{J}\in \mathbb{C}^{N_f N_{out}\times (N_{in}+N_out)(p+1)}$ for the problem as such:
\begin{equation}\label{eq:jacobian}
\mathbf{J} = \begin{pmatrix}
    X_1 & 0 & \dots & 0 & Y_1\\
    0 & X_2 & \dots & 0 & Y_2\\
    \dots & \dots & \dots & \dots & \dots\\
    0 & 0 & 0 & X_{N_{out}} & Y_{N_{out}}
\end{pmatrix}
\end{equation}
The cost function can be represented as:
\begin{equation}\label{eq:cost_fn_simp}
    l^{LS}(\theta) = \mathbf{tr}\{\theta^T \mathbf{J}^H \mathbf{J} \theta \}
\end{equation}
To obtain real values of $\theta$, one must place a constraint on the cost function such that:
\begin{equation}\label{eq:cost_fn_simp_real}
    l^{LS}(\theta) = \mathbf{tr}\{\theta^T \mathit{Re}(\mathbf{J}^H \mathbf{J}) \theta \}
\end{equation}
Where the Gramian matrix of $\mathbf{J}$ can be represented in terms a set of variables, $\mathbf{R}$,$\mathbf{S}$ and $\mathbf{T}$:
\begin{equation}
    \mathit{Re}(\mathbf{J}^H\mathbf{J}) = \begin{pmatrix}
        R_1 & 0 & \dots & 0 & S_1\\
        0& 0 & \dots & 0 & S_2\\
        \dots & \dots & \dots & \dots & \dots\\
        0 & 0 & \dots & R_{N_{out}} & S_{N_{out}}\\
        S^{T}_1 & S^{T}_2 &\dots &S^{T}_{N_{out}} &\sum_{o=1}^{N_{out}}T_o
    \end{pmatrix}
\end{equation}
in which:
\begin{equation}\label{eq:r-tensor}
    R_o = \mathit{Re}(X^{H}_o X_o)
\end{equation}
\begin{equation}\label{eq:s-tensor}
    S_o =\mathit{Re}(X^{H}_o Y_o)
\end{equation}
\begin{equation}\label{eq:t-tensor}
    T_o = \mathit{Re}(Y^{H}_o Y_o)
\end{equation}

\section{The \emph{Normal Equations}, and extracting the modal parameters}
The cost function can then be minimized in terms of $\alpha$ and $\beta$ to find the best least-squares fit:
\begin{equation}
    \begin{aligned}
        \frac{\partial l^{LS}(\theta)}{\partial\beta_o} = 2(R_o \beta_o +S_o\alpha) &= 0 \\
         & \forall O = 1,2,\dots,N_{out}   
    \end{aligned}
\end{equation}
\begin{equation}
    \frac{\partial l^{LS}(\theta)}{\partial\alpha} = 2\sum_{o=1}^{N_{out}}(S^{T}_o\beta_o +T_o \alpha)
\end{equation}
Giving normal equations of this least squares problem in terms of the wanted polynomial coefficients, one can also assemble those normal equations into 1 equation:
\begin{equation}
    \frac{\partial l^{LS}(\theta)}{\partial\theta} = 2\mathit{Re}(\mathbf{J}^H\mathbf{J})\theta = 0
\end{equation}
The denominator coefficients $\alpha$ are used to obtain the poles and the modal participation factors, which are sufficient information for the constructing a stabilization diagram. Hence, one can further reduce the normal equations by setting:
\begin{equation}
    \beta_o = R^{-1}_oS_o\alpha
\end{equation}
This yields the reduced normal equation:
\begin{equation}\label{eq:reduced_normal_eqn}
    \begin{aligned}
        \Biggl\{2\sum_{o=1}^{N_{out}}(T_o - S^{T}_oR^{-1}_oS_o)\Biggr\}\alpha &= 0\\
        \mathbf{M}\alpha & = 0 
    \end{aligned}
\end{equation}
For a non-trivial solution to the normal equation, a constraint is set on $\alpha$, where:
\begin{equation}
    \alpha_p = \mathbf{I}_{N_{in}}
\end{equation}
The rest of the denominator coefficients are then found using:
\begin{equation}
    \mathbf{M}(1:N_{in}*p,1:N_{in}*p)\begin{pmatrix}
        \alpha_0\\ \alpha_1 \\ \alpha_2 \\ \dots \\ \alpha_{p-1}
    \end{pmatrix}
    = \mathbf{M}(1:N_{in}*p,N_{in}*p + 1 : N_{in}*(p+1))
\end{equation}
The least-squares estimate for $\alpha$ is then:
\begin{equation*}
    \hat{\alpha}_{LS} = \begin{Bmatrix}
        \alpha\\ \mathbf{I}_{N_{in}}
    \end{Bmatrix}
\end{equation*}

This makes the denominator polynomial $\mathbf{D}$ a \emph{monic} polynomial. Based on the fundamental definition of system poles, which is the points at which the system's response is "infinite", one can say that for an arbitrary rational polynomial $p(x)$:
\begin{equation}
    p(x) = \frac{a_0 +a_1x+a_2x^2+\dots+a_nx^n}{b_0 +b_1x+b_2x^2+\dots+b_nx^n}
\end{equation}
To achieve said "infinite" value, one must find values of $x$ that represent the roots of the denominator polynomial. In the pLSCF model, one can exploit the monic property of the denominator polynomial. Frobenius companion matrices are square matrices that represent monic polynomials, given one has a monic polynomial
\begin{equation*}
    p(x) = x^n +a_{n-1}x^{n-1}+a_{n-2}x^{n-2}+\dots +a_1x+a_0
\end{equation*}
The companion matrix of said polynomial is defined as:
\begin{equation}
    C(p) = \begin{bmatrix}
        0 & 0 & \dots &0& -a_0\\
        1 & 0 & \dots &0& -a_1\\
        0 & 1 & \dots &0& -a_2\\
        \vdots & \vdots & \ddots & \vdots&\vdots\\
        0 & 0 & \dots & 1 & -a_{n-1}
    \end{bmatrix}
\end{equation}
A property of the companion matrix $C(p)\in \mathbb{R}^{n\times n}$ is that its eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_n$ are the roots of $p(x)$, where $p(\lambda_i) = 0, \forall i =1,2,\dots,n$. This property aids in finding the system poles, after constructing the companion matrix for the denominator polynomial, an Eigendecomposition of the matrix yields the discrete time poles as the eigenvalues, and the corresponding eigenvectors are the modal participation factors:
\begin{equation}
    C(\mathbf{D}) = \begin{pmatrix}
        0 & I &\dots & 0 & 0 \\
        0 & 0 &I & \dots & 0 \\
        \dots & \dots & \dots & \dots & \dots \\
        0 & 0 &\dots & 0 & I \\
        -\alpha_0^T & -\alpha_1^T &-\alpha_2^T& \dots -\alpha_{p-2}^T & -\alpha_{p-1}^T
    \end{pmatrix}
\end{equation}
\begin{equation}
    C(\mathbf{D})[\mathbf{L}] = \Lambda[\mathbf{L}]
\end{equation}
Where the matrix $[\mathbf{L}]$ is the Eigenmatrix (matrix with columns as eigenvectors), representing the modal participation factors of each mode, and the matrix $\Lambda$ contains the discrete time poles on its diagonal elements. The transpose of a companion matrix is used for pLSCF, this however does not affect the numerical value of the poles or participation factors [reference proof]. A $p$-order right matrix-fraction polynomial estimation should yield $pN_{in}$ number of poles.

\section{Finding the modeshapes using the Least-Squares Frequency Domain (LSFD) method}

In practice, modeshapes are estimated using the Least Squares Frequency Domain algorithm, which is a linear regression designed to find the modal residues, a product of the modeshapes and the participation factors, and the upper and lower residuals, values which account for non-ideal vibratory behaviour. The LSFD fits a new model to measured FRF data as shown in Equation \ref{eq:lsfd-model}.

\begin{equation}\label{eq:lsfd-model}
    H(s) = \sum_{m=1}^{N_{modes}}\left( \frac{\Psi_m\cdot L_{m}^{T}}{s - \lambda_m}+\frac{\Psi_{m}^{*}\cdot L_{m}^{H}}{s - \lambda_{m}^{*}} \right) +\frac{\mathbf{LR}}{s^2} + \mathbf{UR}
\end{equation}
Where $\Psi_m$ is the modeshape vector for the mode $m$, $L_{m}$ is the mode's participation factor, $\mathbf{LR}$ and $\mathbf{UR}$ are the upper and lower residuals, and $s = j\omega$. By taking the Modal Residues for the $i^{th}$ mode $[Res_i] = \Psi_m\cdot L_{m}^{T}$, they can be found, alongside the Upper and Lower residuals, in one single linear least squares step. By taking the Jacobian matrix $J$ to be
\begin{equation}
    J = \begin{bmatrix}
        \frac{1}{j\omega_2 - \lambda_1} + \frac{1}{j\omega_2 - \lambda_1^*} & \frac{1}{j\omega_2 - \lambda_2} + \frac{1}{j\omega_2 - \lambda_2^*} & \dots & \frac{1}{j\omega_2 - \lambda_{N_{m}}} + \frac{1}{j\omega_2 - \lambda_{N_{m}}^*} & \frac{-1}{\omega_2^2} & 1 \\

        \frac{1}{j\omega_3 - \lambda_1} + \frac{1}{j\omega_3 - \lambda_1^*} & \frac{1}{j\omega_3 - \lambda_2} + \frac{1}{j\omega_3 - \lambda_2^*} & \dots & \frac{1}{j\omega_3 - \lambda_{N_{m}}} + \frac{1}{j\omega_3 - \lambda_{N_{m}}^*} & \frac{-1}{\omega_2^2} & 1 \\

        \dots & \dots & \dots & \vdots  & \dots &\vdots \\

        \frac{1}{j\omega_{N_{f}} - \lambda_1} + \frac{1}{j\omega_{N_{f}} - \lambda_1^*} & \frac{1}{j\omega_{N_{f}} - \lambda_2} + \frac{1}{j\omega_{N_{f}} - \lambda_2^*} & \dots & \frac{1}{j\omega_{N_{f}} - \lambda_{N_{m}}} + \frac{1}{j\omega_{N_{f}} - \lambda_{N_{m}}^*} & \frac{-1}{\omega_{N_{f}}^2} & 1 \\
    \end{bmatrix}
\end{equation}

The residues and the upper and lower residuals can be estimated as:

\begin{equation}
    \begin{bmatrix}
        Re[Res]_{1o}\\
        \dots\\
        Re[Res]_{N_mo}\\
        Im[Res]_{1o}\\
        \dots\\
        Im[Res]_{N_mo}\\
        [LR]_o\\
        [UR]_o
    \end{bmatrix}
    = Re(J^H J)^{-1} Re(J^H H_o)
    \forall O = 1,2,3,\dots , N_{out}
\end{equation}

With no knowledge of the modal participation factors, the modeshape can only be estimated using a singular value decomposition (SVD) of the residue matrix.
\begin{equation}
    [Res_i] = \mathbf{U\Sigma V^T}
\end{equation}
Where $\mathbf{\Sigma}$ is a diagonal matrix containing the singular values of the matrix, $\mathbf{U}$ is a matrix containing left singular vectors, and $\mathbf{V}$ is a matrix containing right singular vectors. This process can yield accurate modeshapes only if the residue matrix is a rank-1, which mat not always be the case. Knowledge of the modal participation factors, which the pLSCF algorithm provides, removes this limitation by allowing a vector to be calculated using the properties of the outer product and rearranging. The use of the outer product provides quicker computation, as matrix-vector multiplication is generally considered quicker than the SVD, this is particularly important for the frequently encountered, high number of output systems.
\begin{equation}
    \mathbf{\Psi}_i = \frac{[Res_i] \mathbf{L}_i}{\mathbf{L}_i^H \mathbf{L}_i}
\end{equation}




% This part should be in actual report: When fitting theoretical models to experimental data, the difficulty does not typically lie in the mathematical framework enabling the modelling process. Instead, the challenge is in constructing a model that provides physically meaningful insights. Given that the goal of many modal parameter estimation methods is to find a set of poles which As apparent, it is straightforward to find the polynomial coefficients using measured data. 